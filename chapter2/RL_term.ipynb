{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Agent**: 주인공, 학습하는 대상으로, 환경속에서 행동하는 개체를 말한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Environment** : Agent와 상호작용하는 환경. 강화학습은 Agent와 Environment간의 상호작용간에 일어나는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **State** : True configuration of system. Agent가 필요한 구체적 정보(ex. 위치, 속도)를 말한다. Agent가 action을 하면, 그에따라 환경이 agent의 상태를 변화시킴. 흔히 \n",
    "$s$로 표현된다. 또한 일반적으로 markov 가정을 따를 때 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Policy** : Agent가 움직이는 행동 방향. 정책으로도 불리며, 크게 deterministic policy와 stochastic policy로 나뉜다. deterministic policy는 해당 state에 따라 100% 확률로 action이 mapping되는 $a=π(s)$, stochastic policy는 state가 주어졌을 때, action이 확률적으로 선택된다. $a=π(a∣s)$. 파이를 주로 사용함. 강화학습의 목적은 결국 optimal policy (accumulative reward = return을 최대화하는 policy)를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Reward** : Agent가 행동을 했을 때 받게 되는 보상(scalar)으로서, 특정 행동을 유발시키기 위해 positive reward를, 특정 행동을 금지시키기 위해 negative reward를 취할 수 있다. 이때 agent가 어떻게 해야 높은 보상을 받게 되는지 알려주면 안 된다.(에이전트 입장에서 '어라? 이런 시도를 하니까 많이 주네?'로 느껴지게 해야 함). 또한 행동 1번에 꼭 1번 보상이 주어지지 않거나(sparse) 현재 행동이 미래의 보상에 영향을 끼칠 수도 있다.(delayed, ex. 바둑) 미래에 받게 되는 reward에 대해서는 discount을 적용하고, 보상들을 전부 더해 return $G_t$ 를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Return** : $G_t$ 로 표현되고, agent가 time step $t$로부터 앞($t+1$)으로 받게 될 discounted Reward의 누적 합이다. 예.$G_t = R_{t+1}+ rR_{t+2}+ r^2R_{t+3} + ⋯ + r^{T-1}R_T. $ \n",
    "</br>\n",
    "T = Terminal state\n",
    "</br>\n",
    "또한 중요한 것은 Return의 expectaion이 state-value function $v(s) = E[G_t|S_t = s] $ 이라고 한다.\n",
    "</br>\n",
    "여기서 agent의 행동 방식 policy $π$를 고려한다면 $v_π(s)=E_π[G_t∣S_t=s]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exploration** : 탐험. 도박. Agent가 현재 알고 있는 지식으로 행동을 하지 않고, 모르는 분야로 나아가서 정보를 얻기 위해 행동하는 것을 말한다. 예를 들어, 음식점에 갔을 때, 내가 항상 먹던 것을 먹을 것이냐, 안 먹어봤던 신메뉴를 먹어볼 것이냐가 대표적인 예이다. ϵ-greedy에서 ϵ이 모험을 할 확률을 말하며, $Q$가 높은 action을 하는 것이 아닌 모든 action 중 랜덤하게 골라서 action함. model-free한 상황에서 local optimality에 빠지는 걸 방지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Exploitation** : 활용. exploration의 반대. Agent가 아는 지식으로 행동함. Exploration과 상호 대립되는 개념. 강화학습은 Exploration과 Exploitation의 정도를 조절하는 게 중요함. ϵ-greedy에서 greedy하게 action을 선택하는 경우이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Value** : state의 가치를 말함. 가치란 해당 state에 도달했을 때의 reward와 그 state 이후에 얻게 되는 reward를 모두 반영해서 계산됨. \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "v(s)로 표현. agent가 value를 greedy하게 Action을 선택한다고 했을 때, Agent는 이동 가능한 State 중 제일 높은 \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "v(s)로 이동함.\n",
    "이것이 근시안적인 선택으로 보이지만, Value의 정의 자체가 장기적인 가치를 나타내기 때문에, Greedy하게 학습해도 장기적인 보상을 따르는 Agent로 학습이 됨. 보통 \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "v(s)로 표기하면 state-value function이고, \n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "q(s,a)로 표기하면 action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
